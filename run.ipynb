{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import csv\n",
    "import io\n",
    "import datetime\n",
    "\n",
    "pth = r\"C:\\Users\\patrik\\Desktop\\EPFL\\ada\" #change location according to your nltk data path\n",
    "nltk.data.path.append(pth)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 998 Âµs\n"
     ]
    }
   ],
   "source": [
    "data_folder = 'twitter-datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 30 ms\n"
     ]
    }
   ],
   "source": [
    "def read_data(filepath):\n",
    "    tweets       = []\n",
    "    tweets_file  = io.open(data_folder + filepath, 'r', encoding='utf8')\n",
    "\n",
    "    for line in tweets_file:\n",
    "        tw = line.strip()\n",
    "        tweets.append(tw)\n",
    "    tweets = pd.DataFrame(tweets, columns=['tweet'])\n",
    "\n",
    "    return process(tweets)\n",
    "\n",
    "def combine_pos_neg_tweet(pos_file, neg_file):\n",
    "    train_pos = read_data(pos_file)\n",
    "    train_pos['target'] = 1\n",
    "    train_neg = read_data(neg_file)\n",
    "    train_neg['target'] = -1\n",
    "    train_data = pd.concat([train_neg, train_pos]) #merge positive and negative tweets\n",
    "    train_data = train_data.sample(frac=1, random_state=42).reset_index(drop=True) #shuffle the datas\n",
    "\n",
    "    print('we have {} positive tweets, and {} negative ones.'.format(len(train_pos), len(train_neg)))\n",
    "\n",
    "    return train_data\n",
    "\n",
    "def remove_contraction(tweets):\n",
    "\n",
    "    contractions_dict = {\n",
    "        '\\'s': '', 'who\\'s': 'who is', 'i\\'m': 'i am', 'n\\'t': 'n not', 'why\\'s': 'why is', 'he\\'s': 'he is', '\\'ll': ' will',\n",
    "        '\\'l': ' will', 'what\\'s': 'what is', 'when\\'s': 'when is', '\\'re': ' are', '\\'ve': ' have',\n",
    "        '\\'d': ' would', 'how\\'s': 'how is', 'it\\'s': 'it is', 'that\\'s': 'that is', 's\\'': '',\n",
    "    }\n",
    "\n",
    "    pat = re.compile('|'.join(contractions_dict.keys()))\n",
    "    tweets =re.sub(r'<user>','',tweets)\n",
    "    tweets = re.sub(r'<url>','',tweets)\n",
    "    tweets = re.sub(r'rt','',tweets)\n",
    "    tweets = pat.sub(lambda x: contractions_dict[x.group()], tweets)\n",
    "\n",
    "    return tweets\n",
    "\n",
    "def clean_redundant_character(tweet):\n",
    "    cleaned_tweet = ''\n",
    "    words = tweet.split()\n",
    "\n",
    "    for word in words:\n",
    "        cleaned_tweet += re.sub(r'([a-z])\\1+$', r'\\1 <redundant>' , word) + ' '\n",
    "\n",
    "    return cleaned_tweet.strip() #remove whitespace at end\n",
    "\n",
    "def handle_hashtag_number(tweet):\n",
    "    clean_tweet = ''\n",
    "    words     = tweet.split()\n",
    "\n",
    "    for w in words:\n",
    "        try:\n",
    "            remaining = re.sub('[,\\.:%_\\+\\_\\%\\*\\/\\-]', '', w)\n",
    "            float(remaining)\n",
    "            clean_tweet += '<number> '\n",
    "        except:\n",
    "            if w.startswith(\"#\"):\n",
    "                clean_tweet += w[1:] + ' <hashtag> '\n",
    "            else:\n",
    "                clean_tweet += w + ' '\n",
    "\n",
    "    return clean_tweet.strip()\n",
    "\n",
    "def remove_space_punctuation(tweets):\n",
    "    puncts = ['?', '.', '!', '+', '(', ')']\n",
    "    clean_tweets = ''\n",
    "    first = ''\n",
    "    j = 0\n",
    "    for w in tweets.split() :\n",
    "        if(w in puncts) :\n",
    "            if( first != w) :\n",
    "                first = w\n",
    "            else :\n",
    "                j += 1\n",
    "                if( j ==1) :\n",
    "                    clean_tweets += first + ' <repeat> '\n",
    "        else :\n",
    "            clean_tweets += w + ' '\n",
    "    return clean_tweets\n",
    "\n",
    "def map_sentiment(tweet):\n",
    "\n",
    "    cleaned_tweet = ''\n",
    "    words = tweet.split()\n",
    "\n",
    "    #emoji sentiment\n",
    "    for word in words:\n",
    "        if(word in pos_emoji) :\n",
    "            cleaned_tweet += pos_emoji[word] + ' '\n",
    "        elif(word in neg_emoji) :\n",
    "            cleaned_tweet += neg_emoji[word] + ' '\n",
    "        else :\n",
    "            cleaned_tweet += word + ' '\n",
    "\n",
    "    cleaned_tweet = cleaned_tweet.split()\n",
    "\n",
    "    #word sentiment\n",
    "    sentiment_tweet = ''\n",
    "    for word in cleaned_tweet:\n",
    "        if(word in positive_words) :\n",
    "            sentiment_tweet += 'positive ' + word + ' '\n",
    "        elif(word in negative_words) :\n",
    "            sentiment_tweet += 'negative ' + word + ' '\n",
    "        else :\n",
    "            sentiment_tweet += word + ' '\n",
    "    return sentiment_tweet.strip()\n",
    "\n",
    "def stemSentence(tweet):\n",
    "    s = tweet.split()\n",
    "    s = [lemmatizer.lemmatize(x) for x in s]\n",
    "    return \" \".join(s)\n",
    "\n",
    "def process(tweets) :\n",
    "    tweets.tweet = tweets.tweet.apply(lambda t : remove_contraction(t))\n",
    "    tweets.tweet = tweets.tweet.apply(lambda t : map_sentiment(t))\n",
    "    tweets.tweet = tweets.tweet.apply(lambda t : handle_hashtag_number(t))\n",
    "    tweets.tweet = tweets.tweet.apply(lambda t : remove_space_punctuation(t))\n",
    "    tweets.tweet = tweets.tweet.apply(lambda t : clean_redundant_character(t))\n",
    "    tweets.tweet = tweets.tweet.apply(lambda t : stemSentence(t))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8 ms\n"
     ]
    }
   ],
   "source": [
    "def log_reg_helper(X_train, y_train, X_test, max_f = None):\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "    log = LogisticRegression(max_iter=400, penalty='l2', C=10, tol=1e-4, random_state=42)\n",
    "    tfidf_vect = TfidfVectorizer(max_features=max_f, ngram_range=(1,3))\n",
    "    corpus = X_train.append(X_test)\n",
    "    tfidf_vect.fit(corpus)\n",
    "    train_tfidf = tfidf_vect.transform(X_train)\n",
    "    test_tfidf = tfidf_vect.transform(X_test)\n",
    "\n",
    "    duration = datetime.datetime.now() - start_time\n",
    "\n",
    "    print('TFIDF: the corpus fitted and tranformed in {} s'.format(duration.total_seconds()))\n",
    "\n",
    "    log.fit(train_tfidf, y_train)\n",
    "    predict = log.predict(test_tfidf)\n",
    "\n",
    "    duration = datetime.datetime.now() - start_time\n",
    "    print('train finished in {} s'.format(duration.total_seconds()))\n",
    "\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13 ms\n"
     ]
    }
   ],
   "source": [
    "def to_kaggle(id, predicted):\n",
    "    ids = id\n",
    "    with open('logistic_regression_prediction.csv', 'w', newline='') as csvfile:\n",
    "            fieldnames = ['Id', 'Prediction']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for r1, r2 in zip(ids, predicted):\n",
    "                writer.writerow({'Id':int(r1),'Prediction':int(r2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 29 ms\n"
     ]
    }
   ],
   "source": [
    "neg_emoji = io.open('opinion-lexicon-english/negative_emojis.txt', encoding='utf-8-sig').read().split(\"\\n\")\n",
    "neg_emoji = dict([x.split() for x in neg_emoji])\n",
    "pos_emoji = io.open('opinion-lexicon-english/positive_emojis.txt', encoding='utf-8-sig').read().split(\"\\n\")\n",
    "pos_emoji = dict([x.split() for x in pos_emoji])\n",
    "positive_words = set(io.open('opinion-lexicon-english/positive-words.txt', encoding = \"ISO-8859-1\").read().split())\n",
    "negative_words = set(io.open('opinion-lexicon-english/negative-words.txt', encoding = \"ISO-8859-1\").read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_data = combine_pos_neg_tweet('train_pos_full.txt', 'train_neg_full.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = kaggle_data.tweet\n",
    "y_train = kaggle_data.target\n",
    "X_test = kaggle_data.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = log_reg_helper(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_kaggle(X_test.id.values, predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
