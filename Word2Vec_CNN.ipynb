{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.models import load_model\n",
    "import random\n",
    "# Instantly make your loops show a smart progress meter - \n",
    "# just wrap any iterable with tqdm(iterable), and you’re done!\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import KeyedVectors\n",
    "import multiprocessing\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the preprocessed tweets and add label to distinguish between positive and negative ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('twitter-datasets/full_process_train')\n",
    "data['target'] = data['target'].apply(lambda x: 0 if x == -1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.tweet.values.tolist()\n",
    "y = data.target.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive thank you jamally mal ! <repeat> congrats to jer bear for her new job',\n",
       " 'and after that conce you wil <redundant> be boyfriend-les <redundant> ahah . <repeat>',\n",
       " 'finder series <number> : target in the finder yaoi paperback during a routine journalism assignment , akihito takara ...']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **train_test_split** splits arrays or matrices into random train and test subsets\n",
    "- **test_size** : if float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split\n",
    "- **random_state** : if int, random_state is the seed used by the random number generator\n",
    "\n",
    "Here\n",
    "**test_size=.05**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2000\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = \\\n",
    "        train_test_split(X, y, test_size=.05,random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = \\\n",
    "        train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)\n",
    "#list(x_validation_and_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 2116371 entries with 0.00% negative, 0.00% positive\n",
      "Validation set has total 55694 entries with 0.20% negative, 0.20% positive\n",
      "Test set has total 55694 entries with 0.06% negative, 0.06% positive\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train),\n",
    "                (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,                                                                          \n",
    "                (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n",
    "print(\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),\n",
    "                (len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,\n",
    "                (len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100))\n",
    "print(\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_test),\n",
    "                (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n",
    "                (len(x_test[y_test == 1]) / (len(x_test)*1.))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 2116371 entries with 0.00% negative, 0.00% positive\n",
      "Validation set has total 55694 entries with 0.20% negative, 0.20% positive\n",
      "Test set has total 55694 entries with 0.06% negative, 0.06% positive\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train),\n",
    "                                    (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n",
    "                                    (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n",
    "print(\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),\n",
    "                                    (len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,\n",
    "                                    (len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100))\n",
    "print(\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_test),\n",
    "                                    (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n",
    "                                    (len(x_test[y_test == 1]) / (len(x_test)*1.))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function \"home made\" returns a **list** of elements of type **TaggedDocument**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in enumerate(tweets):\n",
    "        result.append(TaggedDocument(t.split(), [prefix + '_%s' % i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We append again **all the data** together and labelize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['such', 'negative', 'harsh', 'word', 'i', 'positive', 'love', 'fuko', 'with', 'al', '<redundant>', 'my', 'hea', '!', '<repeat>'], tags=['all_0']),\n",
       " TaggedDocument(words=['al', '<redundant>', 'the', 'time', ')', '<repeat>'], tags=['all_1']),\n",
       " TaggedDocument(words=['vaultz', 'vz01094', '<number>', '-', 'drawer', 'locking', 'cd', 'storage', 'cabinet', 'hold', 'up', 'to', '<number>', 'cd', 'black', 'with', 'chrome', 'accent', 'lock', 'it', 'up', 'w', '...'], tags=['all_2'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_x = x_train + x_validation + x_test\n",
    "all_x_w2v = labelize(all_x, 'all')\n",
    "#all_x_w2v\n",
    "print(type(all_x_w2v))\n",
    "all_x_w2v[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of abstract model **CBOW** (Continuous Bag Of Words)\n",
    "\n",
    "Parameters:\n",
    "- **size=100**\n",
    "- workers=nb_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "nb_cores = multiprocessing.cpu_count()\n",
    "print(nb_cores)\n",
    "cbow_model = Word2Vec(sg=0, size=100, negative=5, window=2, min_count=2, \n",
    "                         workers=nb_cores, alpha=0.065, min_alpha=0.065)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concretisation of the abstract model CBOW (Continuous Bag Of Words) by associating the correspondant vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2873772.39it/s]\n"
     ]
    }
   ],
   "source": [
    "cbow_model.build_vocab([x.words for x in tqdm(all_x_w2v)])\n",
    "#cbow_model.build_vocab([x.words for x in all_x_w2v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train **CBOW** with the function **train**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2924347.49it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2627977.03it/s]\n",
      "100%|██████████| 2227759/2227759 [00:22<00:00, 97680.29it/s] \n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2441927.41it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2926956.38it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2918282.06it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2924734.68it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2920541.43it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2848379.60it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2734804.09it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2889290.33it/s]\n",
      "100%|██████████| 2227759/2227759 [00:01<00:00, 1842779.92it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2932373.13it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2917156.87it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2940750.19it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2932389.70it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2949213.41it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2774518.48it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2767640.79it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2770031.66it/s]\n",
      "100%|██████████| 2227759/2227759 [00:01<00:00, 1558657.38it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2605226.03it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2950527.44it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2920850.92it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2906839.35it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2897069.66it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2883367.53it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2925937.19it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2919162.77it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2851049.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46min 34s, sys: 12 s, total: 46min 46s\n",
      "Wall time: 14min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    #cbow_model.train(utils.shuffle([x.words for x in all_x_w2v]),\n",
    "    cbow_model.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), \n",
    "                total_examples=len(all_x_w2v), epochs=1)\n",
    "    cbow_model.alpha -= 0.002\n",
    "    cbow_model.min_alpha = cbow_model.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the abstract model **SG** (Skip Gram).\n",
    "\n",
    "\n",
    "Parameters:\n",
    "- **size=100**\n",
    "- workers=nb_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_model = Word2Vec(sg=1, size=100, negative=5, window=2, min_count=2, workers=nb_cores, \n",
    "                       alpha=0.065, min_alpha=0.065)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concretisation of the abstract model SG by associating the correspondant vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2853034.31it/s]\n"
     ]
    }
   ],
   "source": [
    "sg_model.build_vocab([x.words for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train **SG**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2929133.72it/s]\n",
      "100%|██████████| 2227759/2227759 [00:01<00:00, 1840042.28it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2693609.92it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2884437.42it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2367773.82it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2864218.94it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2874364.68it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2568116.99it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2900862.47it/s]\n",
      "100%|██████████| 2227759/2227759 [00:01<00:00, 2102404.33it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2885491.17it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2736100.61it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2603582.55it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2902390.67it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2322299.76it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2906641.33it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2646921.00it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2766629.56it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2931413.61it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2501528.26it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2927347.94it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2804280.32it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2840664.90it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2639355.32it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2805331.05it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2426917.91it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2933045.07it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2777610.53it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2747587.23it/s]\n",
      "100%|██████████| 2227759/2227759 [00:00<00:00, 2475240.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 51min 12s, sys: 14.4 s, total: 1h 51min 27s\n",
      "Wall time: 18min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    sg_model.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), \n",
    "                      total_examples=len(all_x_w2v), epochs=1)\n",
    "    sg_model.alpha -= 0.002\n",
    "    sg_model.min_alpha = sg_model.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the models to reuse them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model.save('cbow_model.word2vec')\n",
    "sg_model.save('sg_model.word2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN (Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model = KeyedVectors.load('cbow_model.word2vec')\n",
    "sg_model = KeyedVectors.load('sg_model.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173940"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cbow_model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get more \"specific\" vectors by concatenating the corresponding vectors obtained with CBOW and SG. The dimension of all the vectors is therefore the sum of the dimensions of BOW and SG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 173940  word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_dico = {}\n",
    "for w in cbow_model.wv.vocab.keys():\n",
    "    embeddings_dico[w] = np.append(cbow_model.wv[w],sg_model.wv[w])\n",
    "print('Found', len(embeddings_dico), ' word vectors.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set **num_words** which give the size of the vocabs that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[41, 1, 6, 61, 12]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428139"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "such negative harsh word i positive love fuko with al <redundant> my hea ! <repeat>\n",
      "al <redundant> the time ) <repeat>\n",
      "vaultz vz01094 <number> - drawer locking cd storage cabinet hold up to <number> cd black with chrome accent lock it up w ...\n",
      "earring stand positive clear <number> <number> \" w x <number> <number> \" d x <number> <number> \" h this revolving earring stand can be used veically or hori ...\n",
      "she is a positive wonderful person & i wudnt mind talkn <number> her but shes gettn positive ready <number> graduate & i always end up sayn sumthin negative stupid wen im wit her\n"
     ]
    }
   ],
   "source": [
    "for x in x_train[:5]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41, 1, 6, 61, 12]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "for x in x_train:\n",
    "    length.append(len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max(length)** gives us the maximum number of words in a sentence within the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we chose **maxlen=127** bigger thatn max(len) calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (2116371, 127)\n"
     ]
    }
   ],
   "source": [
    "x_train_seq = pad_sequences(sequences, maxlen=127)\n",
    "print('Shape of data tensor:', x_train_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0, 41,  1,  6, 61, 12], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_seq[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**maxlen=127**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_val = tokenizer.texts_to_sequences(x_validation)\n",
    "x_val_seq = pad_sequences(sequences_val, maxlen=127)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the variable **num_words = 100000** which specifies the number of words appearing most frequently in the training set and which will be taken into account.\n",
    "\n",
    "Otherwise, all words of the vocabulary will be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 100000\n",
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_dico.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN - Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's create a **\"simple\" CNN model with bigram filters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model with bigram filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use a 1D convolution (better than 2D) with **100** width filters **2** (so we do 2-grams) and with strides of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **100000** because of **num_words = 100000**\n",
    "- **200** is the dimension of the vector\n",
    "- **input_length=127** is **input_length=127** of before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 127, 200)          20000000  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 126, 100)          40100     \n",
      "=================================================================\n",
      "Total params: 20,040,100\n",
      "Trainable params: 20,040,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "structure_test = Sequential()\n",
    "e = Embedding(100000, 200, input_length=127)\n",
    "structure_test.add(e)\n",
    "structure_test.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "structure_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we add **Global Max Pooling** layer, then the pooling layer will extract the maximum value from each filter, and the output dimension will be a just 1-dimensional vector with length as same as the number of filters we applied. This can be directly passed on to a dense layer without flattening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 127, 200)          20000000  \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 126, 100)          40100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 20,040,100\n",
      "Trainable params: 20,040,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "structure_test = Sequential()\n",
    "e = Embedding(100000, 200, input_length=127)\n",
    "structure_test.add(e)\n",
    "structure_test.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "structure_test.add(GlobalMaxPooling1D())\n",
    "structure_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1)\n",
    "Only **weights** parameter (because **trainable** is False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      " - 186s - loss: 0.4034 - acc: 0.8087 - val_loss: 0.3747 - val_acc: 0.8187\n",
      "Epoch 2/5\n",
      " - 876s - loss: 0.3582 - acc: 0.8353 - val_loss: 0.3632 - val_acc: 0.8277\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rootcms/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.908063). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "/home/rootcms/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.712420). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "/home/rootcms/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.373709). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    }
   ],
   "source": [
    "model_cnn_01 = Sequential()\n",
    "#e = Embedding(100000, 200, weights=[embedding_matrix], input_length=127)\n",
    "e = Embedding(100000, 200, weights=[embedding_matrix], input_length=127, trainable = False)\n",
    "model_cnn_01.add(e)\n",
    "model_cnn_01.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_01.add(GlobalMaxPooling1D())\n",
    "model_cnn_01.add(Dense(256, activation='relu'))\n",
    "model_cnn_01.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_01.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn_01.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), \n",
    "                 epochs=4, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2)\n",
    "**No** parameter **weights**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_02 = Sequential()\n",
    "e = Embedding(100000, 200, input_length=127)\n",
    "model_cnn_02.add(e)\n",
    "model_cnn_02.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_02.add(GlobalMaxPooling1D())\n",
    "model_cnn_02.add(Dense(256, activation='relu'))\n",
    "model_cnn_02.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_02.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn_02.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), \n",
    "                 epochs=5, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3)\n",
    "**weights** and **trainable** parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_cnn_03 = Sequential()\n",
    "e = Embedding(100000, 200, weights=[embedding_matrix], input_length=127, trainable=True)\n",
    "model_cnn_03.add(e)\n",
    "model_cnn_03.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_03.add(GlobalMaxPooling1D())\n",
    "model_cnn_03.add(Dense(256, activation='relu'))\n",
    "model_cnn_03.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_03.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn_03.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), \n",
    "                 epochs=5, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN more elaborate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are improving the \"simple\" CNN model with bigram filters defined above.\n",
    "\n",
    "Specifically, we combine several 1Convolutions of 2grams, 3grams and 4 grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tweet_input = Input(shape=(**127**,), dtype='int32')\n",
    "- tweet_encoder = Embedding(**100000**, **200**, weights=[embedding_matrix], input_length=**127**, trainable=True)(tweet_input)\n",
    "- bigram_branch = Conv1D(filters=**100**, kernel_size=2, padding='valid', activation='relu', strides=**1**)(tweet_encoder)                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 127)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 127, 200)     20000000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 126, 100)     40100       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 125, 100)     60100       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 124, 100)     80100       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 100)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 100)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 100)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 300)          0           global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            257         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 20,257,613\n",
      "Trainable params: 20,257,613\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.models import Model\n",
    "\n",
    "tweet_input = Input(shape=(127,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(100000, 200, weights=[embedding_matrix], \n",
    "                          input_length=127, trainable=True)(tweet_input)\n",
    "bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', \n",
    "                       activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', \n",
    "                        activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', \n",
    "                         activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = Dense(1)(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2116371 samples, validate on 55694 samples\n",
      "Epoch 1/5\n",
      "1646656/2116371 [======================>.......] - ETA: 1:05:08 - loss: 0.3729 - acc: 0.8306"
     ]
    }
   ],
   "source": [
    "filepath=\"CNN_best_weights.{epoch:02d}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.fit(x_train_seq, y_train, batch_size=32, epochs=5,\n",
    "                     validation_data=(x_val_seq, y_validation), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that the second epoch had the better val_acc (validation accuracy, accuracy of data that the model never see) in tweets that  so we use it next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_CNN_model = load_model('CNN_best_weights.02.hdf5')\n",
    "loaded_CNN_model.evaluate(x=x_val_seq, y=y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
